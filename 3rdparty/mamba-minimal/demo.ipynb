{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531467a2-5160-4073-a990-0d81d574b014",
   "metadata": {},
   "source": [
    "## (1) Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9337043-4e7a-4b20-9d89-6c6257245334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mamba/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "# pretrained_model_name = 'state-spaces/mamba-370m'\n",
    "pretrained_model_name = '/share/public_models/mamba-2.8b'\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69eee709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba(\n",
      "  (embedding): Embedding(50280, 2560)\n",
      "  (layers): ModuleList(\n",
      "    (0-63): 64 x ResidualBlock(\n",
      "      (mixer): MambaBlock(\n",
      "        (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "        (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
      "        (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
      "        (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
      "        (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm_f): RMSNorm()\n",
      "  (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2efb17-37ad-472b-b029-9567acf17629",
   "metadata": {},
   "source": [
    "## (2) Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b2d62d-0d95-4a3f-bd98-aa37e3f26b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "             n_tokens_to_gen: int = 50,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40):\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "    \n",
    "    for token_n in range(n_tokens_to_gen):\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            next_token_logits = model(indices_to_input)[:, -1]\n",
    "        \n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "        \n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "    output_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "    \n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee877143-2042-4579-8042-a96db6200517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba is the Mamba\"\n",
      "\n",
      "\"How about your brother\" was the question from the reporter.\n",
      "\n",
      "\"Oh he's gone to Paris, my brother has been to Paris many times, my brother goes\n",
      "to Paris on business and Paris is the only\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'Mamba is the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d70549-597f-49ca-9185-2184d2576f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John: Hi!\n",
      "Sally: Hi.\n",
      "John: How's your momm--\n",
      "John: How's your--\n",
      "(John and Sally scream together)\n",
      "John: It's okay, I know, it's--\n",
      "I--\n",
      "Sally: Stop it, you\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'John: Hi!\\nSally:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d419fc9-066b-4818-812c-2f1952528bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is ~~~~~to stop worrying and then enjoy life.\n",
      "\n",
      "The three most important things are ~~~~\n",
      "\n",
      "You cannot use your intellect\n",
      "\n",
      "You cannot judge the time.\n",
      "\n",
      "You cannot judge the place.\n",
      "\n",
      "You are always one step\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'The meaning of life is '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b189e6e-6a96-4770-88cf-7c5de22cb321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reverse_string(string):\n",
      "    # Write code here\n",
      "    return string.reverse()\n",
      "\n",
      "# Write your code here\n",
      "def reverse_string2(string):\n",
      "    for i in range(len(string)):\n",
      "        string[i] = reverse\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'def reverse_string('))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3afb51-5093-4c64-ac3f-43c2e6b20b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My cat wrote all this CUDA code for a new language model and I gave it to my friend, I know it is written in CUDA. But, I see the following warning when running it:\n",
      "\n",
      "OpenCL: failed to create a device object (error: 3, e.g. No such device\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'My cat wrote all this CUDA code for a new language model and'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531acc0-b18f-472a-8e99-cee64dd51cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efe197-891a-4ab8-8cea-413d1fb1acda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99509b-df7b-4bac-b6a2-669f601ec1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
